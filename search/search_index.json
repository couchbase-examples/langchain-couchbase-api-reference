{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"LangChain Couchbase API Reference","text":"<p>This document provides a comprehensive reference for the <code>langchain-couchbase</code> package, which integrates LangChain with Couchbase.</p>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install -U langchain-couchbase\n</code></pre>"},{"location":"#table-of-contents","title":"Table of Contents","text":"<ul> <li>CouchbaseVectorStore</li> <li>CouchbaseCache</li> <li>CouchbaseSemanticCache</li> <li>CouchbaseChatMessageHistory</li> </ul>"},{"location":"#couchbasevectorstore","title":"CouchbaseVectorStore","text":"<p><code>CouchbaseVectorStore</code> enables the usage of Couchbase for Vector Search.</p>"},{"location":"#import","title":"Import","text":"<pre><code>from langchain_couchbase import CouchbaseVectorStore\n</code></pre>"},{"location":"#constructor-parameters","title":"Constructor Parameters","text":"Parameter Type Required Default Description cluster Cluster Yes - Couchbase cluster object with active connection bucket_name str Yes - Name of bucket to store documents in scope_name str Yes - Name of scope in the bucket to store documents in collection_name str Yes - Name of collection in the scope to store documents in embedding Embeddings Yes - Embedding function to use index_name str Yes - Name of the Search index to use text_key str No \"text\" Key in document to use as text embedding_key str No \"embedding\" Key in document to use for the embeddings scoped_index bool No True Specify whether the index is a scoped index"},{"location":"#key-methods","title":"Key Methods","text":""},{"location":"#add_texts","title":"<code>add_texts</code>","text":"<p>Add texts to the vector store.</p> <pre><code>def add_texts(\n    self,\n    texts: Iterable[str],\n    metadatas: Optional[List[dict]] = None,\n    ids: Optional[List[str]] = None,\n    batch_size: Optional[int] = None,\n    **kwargs: Any,\n) -&gt; List[str]\n</code></pre>"},{"location":"#similarity_search","title":"<code>similarity_search</code>","text":"<p>Return documents most similar to the query.</p> <pre><code>def similarity_search(\n    self,\n    query: str,\n    k: int = 4,\n    search_options: Optional[Dict[str, Any]] = {},\n    **kwargs: Any,\n) -&gt; List[Document]\n</code></pre>"},{"location":"#similarity_search_with_score","title":"<code>similarity_search_with_score</code>","text":"<p>Return documents most similar to the query with their scores.</p> <pre><code>def similarity_search_with_score(\n    self,\n    query: str,\n    k: int = 4,\n    search_options: Optional[Dict[str, Any]] = {},\n    **kwargs: Any,\n) -&gt; List[Tuple[Document, float]]\n</code></pre>"},{"location":"#delete","title":"<code>delete</code>","text":"<p>Delete documents from the vector store by IDs.</p> <pre><code>def delete(self, ids: Optional[List[str]] = None, **kwargs: Any) -&gt; Optional[bool]\n</code></pre>"},{"location":"#usage-example","title":"Usage Example","text":"<pre><code>import getpass\nfrom datetime import timedelta\nfrom langchain_openai import OpenAIEmbeddings\nfrom couchbase.auth import PasswordAuthenticator\nfrom couchbase.cluster import Cluster\nfrom couchbase.options import ClusterOptions\nfrom langchain_couchbase import CouchbaseVectorStore\n\n# Constants for the connection\nCOUCHBASE_CONNECTION_STRING = getpass.getpass(\"Enter the connection string for the Couchbase cluster: \")\nDB_USERNAME = getpass.getpass(\"Enter the username for the Couchbase cluster: \")\nDB_PASSWORD = getpass.getpass(\"Enter the password for the Couchbase cluster: \")\nBUCKET_NAME = \"langchain_bucket\"\nSCOPE_NAME = \"_default\"\nCOLLECTION_NAME = \"default\"\nSEARCH_INDEX_NAME = \"langchain-test-index\"\n\n# Create Couchbase connection object\nauth = PasswordAuthenticator(DB_USERNAME, DB_PASSWORD)\noptions = ClusterOptions(auth)\ncluster = Cluster(COUCHBASE_CONNECTION_STRING, options)\n\n# Wait until the cluster is ready for use.\ncluster.wait_until_ready(timedelta(seconds=5))\n\n# Initialize embeddings\nembeddings = OpenAIEmbeddings()\n\n# Create vector store\nvector_store = CouchbaseVectorStore(\n    cluster=cluster,\n    bucket_name=BUCKET_NAME,\n    scope_name=SCOPE_NAME,\n    collection_name=COLLECTION_NAME,\n    embedding=embeddings,\n    index_name=SEARCH_INDEX_NAME,\n)\n\n# Add documents\nfrom langchain_core.documents import Document\ndocument_1 = Document(page_content=\"foo\", metadata={\"baz\": \"bar\"})\ndocument_2 = Document(page_content=\"thud\", metadata={\"bar\": \"baz\"})\nvector_store.add_documents([document_1, document_2])\n\n# Search\nresults = vector_store.similarity_search(\"thud\", k=1)\nfor doc in results:\n    print(f\"* {doc.page_content} [{doc.metadata}]\")\n\n# Search with score\nresults = vector_store.similarity_search_with_score(\"thud\", k=1)\nfor doc, score in results:\n    print(f\"* [SIM={score:3f}] {doc.page_content} [{doc.metadata}]\")\n</code></pre>"},{"location":"#couchbasecache","title":"CouchbaseCache","text":"<p><code>CouchbaseCache</code> allows using Couchbase as a cache for prompts and responses.</p>"},{"location":"#import_1","title":"Import","text":"<pre><code>from langchain_couchbase.cache import CouchbaseCache\n</code></pre>"},{"location":"#constructor-parameters_1","title":"Constructor Parameters","text":"Parameter Type Required Default Description cluster Cluster Yes - Couchbase cluster object with active connection bucket_name str Yes - Name of the bucket to store documents in scope_name str Yes - Name of the scope in bucket to store documents in collection_name str Yes - Name of the collection in the scope to store documents in ttl Optional[timedelta] No None Time to live for the document in the cache"},{"location":"#key-methods_1","title":"Key Methods","text":""},{"location":"#lookup","title":"<code>lookup</code>","text":"<p>Look up from cache based on prompt and llm_string.</p> <pre><code>def lookup(self, prompt: str, llm_string: str) -&gt; Optional[RETURN_VAL_TYPE]\n</code></pre>"},{"location":"#update","title":"<code>update</code>","text":"<p>Update cache based on prompt and llm_string.</p> <pre><code>def update(self, prompt: str, llm_string: str, return_val: RETURN_VAL_TYPE) -&gt; None\n</code></pre>"},{"location":"#clear","title":"<code>clear</code>","text":"<p>Clear the cache.</p> <pre><code>def clear(self, **kwargs: Any) -&gt; None\n</code></pre>"},{"location":"#usage-example_1","title":"Usage Example","text":"<pre><code>from datetime import timedelta\nfrom langchain_core.globals import set_llm_cache\nfrom couchbase.auth import PasswordAuthenticator\nfrom couchbase.cluster import Cluster\nfrom couchbase.options import ClusterOptions\nfrom langchain_couchbase.cache import CouchbaseCache\n\n# Create Couchbase connection object\nauth = PasswordAuthenticator(DB_USERNAME, DB_PASSWORD)\noptions = ClusterOptions(auth)\ncluster = Cluster(COUCHBASE_CONNECTION_STRING, options)\n\n# Wait until the cluster is ready for use.\ncluster.wait_until_ready(timedelta(seconds=5))\n\n# Set up the cache\nset_llm_cache(\n    CouchbaseCache(\n        cluster=cluster,\n        bucket_name=BUCKET_NAME,\n        scope_name=SCOPE_NAME,\n        collection_name=COLLECTION_NAME,\n    )\n)\n\n# Now any LLM calls will use the cache\nfrom langchain_openai import OpenAI\nllm = OpenAI(temperature=0)\n# First call will be executed and cached\nresult1 = llm.invoke(\"What is the capital of France?\")\n# Second call with the same prompt will be retrieved from cache\nresult2 = llm.invoke(\"What is the capital of France?\")\n</code></pre>"},{"location":"#couchbasesemanticcache","title":"CouchbaseSemanticCache","text":"<p><code>CouchbaseSemanticCache</code> allows retrieving cached prompts based on the semantic similarity between the user input and previously cached inputs.</p>"},{"location":"#import_2","title":"Import","text":"<pre><code>from langchain_couchbase.cache import CouchbaseSemanticCache\n</code></pre>"},{"location":"#constructor-parameters_2","title":"Constructor Parameters","text":"Parameter Type Required Default Description cluster Cluster Yes - Couchbase cluster object with active connection embedding Embeddings Yes - Embedding model to use bucket_name str Yes - Name of the bucket to store documents in scope_name str Yes - Name of the scope in bucket to store documents in collection_name str Yes - Name of the collection in the scope to store documents in index_name str Yes - Name of the Search index to use score_threshold Optional[float] No None Score threshold to use for filtering results ttl Optional[timedelta] No None Time to live for the document in the cache"},{"location":"#key-methods_2","title":"Key Methods","text":""},{"location":"#lookup_1","title":"<code>lookup</code>","text":"<p>Look up from cache based on the semantic similarity of the prompt.</p> <pre><code>def lookup(self, prompt: str, llm_string: str) -&gt; Optional[RETURN_VAL_TYPE]\n</code></pre>"},{"location":"#update_1","title":"<code>update</code>","text":"<p>Update cache based on the prompt and llm_string.</p> <pre><code>def update(self, prompt: str, llm_string: str, return_val: RETURN_VAL_TYPE) -&gt; None\n</code></pre>"},{"location":"#clear_1","title":"<code>clear</code>","text":"<p>Clear the cache.</p> <pre><code>def clear(self, **kwargs: Any) -&gt; None\n</code></pre>"},{"location":"#usage-example_2","title":"Usage Example","text":"<pre><code>from datetime import timedelta\nfrom langchain_core.globals import set_llm_cache\nfrom langchain_openai import OpenAIEmbeddings\nfrom couchbase.auth import PasswordAuthenticator\nfrom couchbase.cluster import Cluster\nfrom couchbase.options import ClusterOptions\nfrom langchain_couchbase.cache import CouchbaseSemanticCache\n\n# Create Couchbase connection object\nauth = PasswordAuthenticator(DB_USERNAME, DB_PASSWORD)\noptions = ClusterOptions(auth)\ncluster = Cluster(COUCHBASE_CONNECTION_STRING, options)\n\n# Wait until the cluster is ready for use.\ncluster.wait_until_ready(timedelta(seconds=5))\n\n# Initialize embeddings\nembeddings = OpenAIEmbeddings()\n\n# Set up the semantic cache\nset_llm_cache(\n    CouchbaseSemanticCache(\n        cluster=cluster,\n        embedding=embeddings,\n        bucket_name=BUCKET_NAME,\n        scope_name=SCOPE_NAME,\n        collection_name=COLLECTION_NAME,\n        index_name=INDEX_NAME,\n    )\n)\n\n# Now any LLM calls will use the semantic cache\nfrom langchain_openai import OpenAI\nllm = OpenAI(temperature=0)\n# First call will be executed and cached\nresult1 = llm.invoke(\"What is the capital of France?\")\n# Second call with a semantically similar prompt will be retrieved from cache\nresult2 = llm.invoke(\"Tell me the capital city of France\")\n</code></pre>"},{"location":"#couchbasechatmessagehistory","title":"CouchbaseChatMessageHistory","text":"<p><code>CouchbaseChatMessageHistory</code> allows using Couchbase as the storage for chat messages.</p>"},{"location":"#import_3","title":"Import","text":"<pre><code>from langchain_couchbase.chat_message_histories import CouchbaseChatMessageHistory\n</code></pre>"},{"location":"#constructor-parameters_3","title":"Constructor Parameters","text":"Parameter Type Required Default Description cluster Cluster Yes - Couchbase cluster object with active connection bucket_name str Yes - Name of the bucket to store documents in scope_name str Yes - Name of the scope in bucket to store documents in collection_name str Yes - Name of the collection in the scope to store documents in session_id str Yes - Value for the session used to associate messages from a single chat session session_id_key str No \"session_id\" Name of the field to use for the session id message_key str No \"message\" Name of the field to use for the messages create_index bool No True Create an index if True ttl Optional[timedelta] No None Time to live for the documents in the collection"},{"location":"#key-methods_3","title":"Key Methods","text":""},{"location":"#add_message","title":"<code>add_message</code>","text":"<p>Add a message to the chat history.</p> <pre><code>def add_message(self, message: BaseMessage) -&gt; None\n</code></pre>"},{"location":"#add_messages","title":"<code>add_messages</code>","text":"<p>Add multiple messages to the chat history in a batched manner.</p> <pre><code>def add_messages(self, messages: Sequence[BaseMessage]) -&gt; None\n</code></pre>"},{"location":"#clear_2","title":"<code>clear</code>","text":"<p>Clear the chat history.</p> <pre><code>def clear(self) -&gt; None\n</code></pre>"},{"location":"#messages-property","title":"<code>messages</code> (property)","text":"<p>Get all messages in the chat history associated with the session_id.</p> <pre><code>@property\ndef messages(self) -&gt; List[BaseMessage]\n</code></pre>"},{"location":"#usage-example_3","title":"Usage Example","text":"<pre><code>from datetime import timedelta\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain_core.messages import AIMessage, HumanMessage\nfrom couchbase.auth import PasswordAuthenticator\nfrom couchbase.cluster import Cluster\nfrom couchbase.options import ClusterOptions\nfrom langchain_couchbase.chat_message_histories import CouchbaseChatMessageHistory\n\n# Create Couchbase connection object\nauth = PasswordAuthenticator(DB_USERNAME, DB_PASSWORD)\noptions = ClusterOptions(auth)\ncluster = Cluster(COUCHBASE_CONNECTION_STRING, options)\n\n# Wait until the cluster is ready for use.\ncluster.wait_until_ready(timedelta(seconds=5))\n\n# Create chat message history\nmessage_history = CouchbaseChatMessageHistory(\n    cluster=cluster,\n    bucket_name=BUCKET_NAME,\n    scope_name=SCOPE_NAME,\n    collection_name=COLLECTION_NAME,\n    session_id=\"test-session\",\n)\n\n# Create memory with the message history\nmemory = ConversationBufferMemory(\n    memory_key=\"chat_history\",\n    chat_memory=message_history,\n    return_messages=True\n)\n\n# Add messages\nmessage_history.add_user_message(\"Hello, how are you?\")\nmessage_history.add_ai_message(\"I'm doing well, thank you for asking!\")\n\n# Add multiple messages at once\nmessages = [\n    HumanMessage(content=\"What can you help me with today?\"),\n    AIMessage(content=\"I can help you with a variety of tasks. What do you need assistance with?\")\n]\nmessage_history.add_messages(messages)\n\n# Retrieve all messages\nall_messages = message_history.messages\nfor message in all_messages:\n    print(f\"{message.type}: {message.content}\")\n\n# Clear the history\nmessage_history.clear()\n</code></pre>"}]}